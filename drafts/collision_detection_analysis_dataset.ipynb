{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd177b6-ccdf-4f43-818e-acd3ed5c524b",
   "metadata": {},
   "source": [
    "# Collision detection analysis - dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0c5c6-4d32-4cbe-8f4c-2e338912cd15",
   "metadata": {},
   "source": [
    "TODO move this to doc\n",
    "\n",
    "## Iterative collision detection for implicit surfaces\n",
    "\n",
    "This is a detailed description of the collision detection method used in Torch Lens Maker. First, a bit of nomemclature:\n",
    "\n",
    "* \"algorithm\" refers top one instance of Newton, Gradient Descent or Levenberg-Marquardt with associated configuration like number of iterations, maximum step size, damping parameter, etc.\n",
    "* \"method\" refers to the overall collision detection procedure, which includes three phases which each use a single algorithm.\n",
    "\n",
    "**Step 1: Initialization**\n",
    "\n",
    "Initialize t values. Different initialization methods are available. During the optimization, each ray can be associated with multiple t values so that the search can progress from multiple starting values of t in parallel. This is akin to particle optimization, but here the search is quite simple it's one dimensional. Each of these is called a \"beam\" in the source code. \n",
    "\n",
    "So ultimately tensors in the code can have three dimensions:\n",
    "* N, the number of rays\n",
    "* H, the number of iteration steps\n",
    "* B, the number of beams per ray\n",
    "\n",
    "**Step 2: Coarse phase**\n",
    "\n",
    "Run a fixed number of steps of algorithm A, with B beams for each ray. The goal here is to have at least one beam within a close distance to the global minimum.\n",
    "\n",
    "**Step 3: Fine phase**\n",
    "\n",
    "Starting from the best beam of the coarse phase, run a fixed number of steps of algorithm B with a single beam. The goal here is to refine the solution to a high degree of precision.\n",
    "\n",
    "**Step 4: Differentiable step**\n",
    "\n",
    "Run a single step of algorithm C. The goal here is to provide differentiability during torch backwards pass. Every step except this one is run under `torch.no_grad()`.\n",
    "\n",
    "## Choosing the inner algorithm and their parameters\n",
    "\n",
    "We want to use Newton because it has the fastest convergence. But it has one problem, it's undefined when the dot product is zero. This can happen quite frequently when the surface normal and the ray unit vector are orthogonal. To work around this, we use LM with a small damping factor, like 0.1. That way, when the dot product is close to zero, it's closer to gradient descent. When the dot product is far from zero, it's closer to Newton's method. The damping value also prevents overshooting the target and helps with discontinuities in the implicit function.\n",
    "\n",
    "Use beam search and bbox sampling initialization to find global minimum and avoid oscilation or convergence to a local one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c04d0de-c445-461c-8732-1b4ea79fc116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
